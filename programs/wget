# -*- sh -*-

# ensure $HOME/.cookies.txt exists, otherwise wget complains
[ -f $HOME/.cookies.txt ] || touch $HOME/.cookies.txt

# gets single URL
function geturl {
    wget --quiet --load-cookies $HOME/.cookies.txt "$@"
}    

# gets single URL, preserving URL in filename
function geturlx {
    wget --quiet --load-cookies $HOME/.cookies.txt -x "$@"
}

# gets single URL, outputs to stdout
function getcat {
    wget --quiet --load-cookies $HOME/.cookies.txt -O - "$@"
}

# gets all URLs referenced on a *single* page
function getpage {
#    wget --quiet --load-cookies $HOME/.cookies.txt --follow-ftp --recursive --convert-links --timeout=300 -nv --timestamping --span-hosts --level=1 "$@"
    # from wget manpage (see -p)
    wget --quiet --no-directories --load-cookies $HOME/.cookies.txt --follow-ftp --html-extension --span-hosts --convert-links --page-requisites "$@"
}    

# recursively get all URLs referenced on a single page, 
# without ascending to parent directory (careful!)
function getsite {
    wget --quiet --load-cookies $HOME/.cookies.txt --html-extension --follow-ftp --recursive --convert-links --timeout=300 -nv --timestamping --no-parent "$@"
}

# retrieves all URLs linked to from specified URL that have the given
# extension.  Useful for MP3s, etc.
function getcontent {

    accept=$1
    shift
    
    if [ $# == 0 ]; then
        echo "usage: getcontent accept url [url...]"
        return 0
    fi
    
    wget --quiet --load-cookies $HOME/.cookies.txt --recursive --span-hosts --level=1 --no-directories --accept "$accept" "$@"
}
